{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf , SparkContext\n",
    "import operator\n",
    "import binascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡開始這次作業Shingling的部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mapper1主要在執行讀取資料，並限制資料讀取時開頭必須不能是\"\"(排除空行)，再利用ascii code區間，取得開頭為數字或字母的單詞(排除標點)，並在append時讓key值為1，方便之後能reduceByKey。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper1(line):\n",
    "    wordlist = line.split(\" \")\n",
    "    maplist = []\n",
    "    for word in wordlist:\n",
    "        if word!='':\n",
    "            if ((ord(word[0])>=48 and ord(word[0])<=57) or (ord(word[0])>=65 and ord(word[0])<=90) or (ord(word[0])>=97 and ord(word[0])<=122)):\n",
    "                maplist.append((1,[word]))\n",
    "    return maplist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reducer1主要進行list相加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer1(list1,list2):\n",
    "    return list1+list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mapper2主要在形成3-shingle，由於是3-shingle，所以for-loop從0到單詞數量-2形成shingles並append，最後回傳的即為shingles的list。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper2(line):\n",
    "    wordlist = line[1]\n",
    "    maplist = []\n",
    "    n = len(wordlist)\n",
    "    for i in range(n-2):\n",
    "        shingle = wordlist[i:i + 3]\n",
    "        shingle = ' '.join(shingle)\n",
    "        maplist.append(shingle)\n",
    "    return maplist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mapper3主要在做shingles的hash，hash方法先將shingle encode使其能轉為binary ascii code，並and 0xffffffff，最後回傳hash過後的shingles list。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper3(line):\n",
    "    shingle = line.encode()\n",
    "    maplist = []\n",
    "    crc = binascii.crc32(shingle) & 0xffffffff\n",
    "    maplist.append(crc)\n",
    "    return maplist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡在定義sc，且開一個大小為101的陣列docsAsShingleSets並設為global，後面用來儲存各個documents的shingleSet。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"LSH\")\n",
    "sc = SparkContext(conf=conf)\n",
    "docsAsShingleSets = [0]*101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡利用for-loop來讀取這101個document，每個document先做flatMap(mapper1)來讀取資料，執行reduceByKey後將所有讀取進來的單詞合成一個line，再執行flatMap(mapper2)，形成3-shingles，最後執行flatMap(mapper3)對所有shingles進行hash。後面先append(i+1)目的是要將document的編號append進去，再將hash完的結果take出來，append每個hash完的shingles(在裡面進行判斷，如無重複才進行append)，最後將hash完且整理過後(無重複)的list存到該document的ShingleSet。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,101):\n",
    "    if i>=0 and i<=8:\n",
    "        txt = '00'+str(i+1)+'.txt'\n",
    "    elif i>=9 and i<=98:\n",
    "        txt = '0'+str(i+1)+'.txt'\n",
    "    else:\n",
    "        txt = str(i+1)+'.txt'\n",
    "    lines = sc.textFile(txt).flatMap(mapper1)\n",
    "    lines = lines.reduceByKey(reducer1)\n",
    "    lines = lines.flatMap(mapper2)\n",
    "    lines = lines.flatMap(mapper3)\n",
    "    shinglesInDocInts = []\n",
    "    n = lines.count()\n",
    "    lines = lines.take(n)\n",
    "    shinglesInDocInts.append(i+1)\n",
    "    for crc in lines:\n",
    "        if crc not in shinglesInDocInts:\n",
    "            shinglesInDocInts.append(crc)\n",
    "    docsAsShingleSets[i] = shinglesInDocInts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下開始進行這次作業Minhashing的部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡利用一個list1來存101個documents hash完的shingle value，利用for-loop先將第一格排除(第一格為document id)，確認是否已經存在於list1，如果沒有，append到list1，做完101次後就能形成所有shingle value的list，且裡面的元素經過sort()後由小排到大，每個元素互不重複。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list1 = []\n",
    "for i in range(0,101):\n",
    "    docsAsShingleSets[i].sort()\n",
    "    for a in docsAsShingleSets[i]:\n",
    "        if a!=docsAsShingleSets[i][0]:\n",
    "            if a not in list1:\n",
    "                list1.append(a)\n",
    "list1.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mapper4主要在幫助形成類似於shingle X document matrix的list，用for-loop將line裡面除了document id的element依據他在list1的位置，\n",
    "append(document id,[位置])到要return的list(需append document id是因為之後需reduceByKey)，而位置的概念即與shingle X document matrix值為1的位置相同，由於只有值為1時需更新，因此我們只做出值為1的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper4(line):\n",
    "    maplist = []\n",
    "    doc = line[0]\n",
    "    for ele in line:\n",
    "        if ele!=doc:\n",
    "            maplist.append((doc,[list1.index(ele)]))\n",
    "    return maplist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mapper5主要在利用100個hash function做出類似於signature matrix的list，利用for-loop跑100次，找出在hash function裡的最小值，並append 100個hash functions的最小值及document id到return list，使其形成一個類似於signature matrix的list。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper5(line):\n",
    "    doc = line[0]\n",
    "    row = line[1]\n",
    "    list_sig = []\n",
    "    maplist = []\n",
    "    for i in range(100):\n",
    "        mini = 30000\n",
    "        for ele in row:\n",
    "            if (h[i][ele]<mini):\n",
    "                mini = h[i][ele]\n",
    "        list_sig.append(mini)\n",
    "    maplist.append((doc,list_sig))\n",
    "    return maplist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡先將docsAsShingleSets做sc.parallelize使其成為RDD的型態來做mapper及reducer，先執行flatMap(mapper4)，幫助形成類似於shingle X document matrix的list，再執行reduceByKey(reducer1)，使其真正形成類似於shingle X document matrix的lines。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "line_min = sc.parallelize(docsAsShingleSets)\n",
    "line_min = line_min.flatMap(mapper4)\n",
    "line_min = line_min.reduceByKey(reducer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡先做出100個hash function，由於知道前面list1的長度為26503，於是做一個100＊26503的二維陣列作為hash function，而hash的定義為((2+i)*j+1)%26503，0<=i<=99，0<=j<=26502，接著執行flatMap(mapper5)，利用hash functions形成一個類似於signature matrix的line。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [[0]*26503 for i in range(100)]\n",
    "for i in range(100):\n",
    "    for j in range(26503):\n",
    "        h[i][j] = ((2+i)*j+1)%26503\n",
    "line_min = line_min.flatMap(mapper5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下開始進行這次作業Locality-sensitive hashing的部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mapper6主要將50個band裡面的column hash到buckets，利用for-loop算出各column的hash value(由於row=2，hash_value定義為第一個row值＊2+第二個row值＊5)，最後append((#band, hash_value), [doc])，(#band, hash_value)方便之後做reduce(需找出同個band在同個bucket的ducuments)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper6(line):\n",
    "    doc = line[0]\n",
    "    sig = line[1]\n",
    "    bucket = []\n",
    "    for band in range(50):\n",
    "        hash_value = (sig[2*band]*2+sig[2*band+1]*5)\n",
    "        bucket.append(((band,hash_value), [doc]))\n",
    "    return bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mapper7主要找出可以形成candidate pairs的documents，如果在同個bucket有超過2個document(len(doc)>=2)，則可以形成candidate pair，append(個數,documents id)到list。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper7(line):\n",
    "    bukval = line[0]\n",
    "    doc = line[1]\n",
    "    similar = []\n",
    "    if len(doc)>=2:\n",
    "        similar.append((len(doc), doc))\n",
    "    return similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mapper8主要在形成candidate pairs，由於在同個buckets不一定只有2個documents，所以利用雙層for-loop來做，形成(document個數＊(document個數-1))/2個cadidate pairs，最後append(pair)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper8(line):\n",
    "    num = line[0]\n",
    "    doc = line[1]\n",
    "    similar = []\n",
    "    for i in range(num):\n",
    "        for j in range(i+1,num):\n",
    "            pair = [doc[i],doc[j]]\n",
    "            similar.append(pair)\n",
    "    return similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mapper9主要在算出各cadidate pair的similarity，利用signature list找出相同的個數除以100 (100 hash functions)後即為此pair的similarity，最後append((similarity, pair))。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper9(line):\n",
    "    doc1 = line[0]\n",
    "    doc2 = line[1]\n",
    "    similarity = []\n",
    "    same = 0\n",
    "    for i in range(100):\n",
    "        if signature[doc1-1][1][i]==signature[doc2-1][1][i]:\n",
    "            same+=1\n",
    "    similarity.append((same/100, line))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡先執行flatMap(mapper6)將bands裡的column hash到buckets裡，再執行reduceByKey(reducer1)將位於同個bucket的documents reduce在一起，再執行flatMap(mapper7)找出可以形成candidate pairs的documents，接著執行flatMap(mapper8)做出candidate pairs。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "line_lsh = line_min.flatMap(mapper6)\n",
    "line_lsh = line_lsh.reduceByKey(reducer1)\n",
    "line_sim = line_lsh.flatMap(mapper7)\n",
    "line_sim = line_sim.flatMap(mapper8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡主要在過濾重複的candidate pairs，建立一個新的similar_pair的list，將所有pairs take出來後利用for-loop把不存在於similar_pair的pair append進去，最後形成的similar_pair list即為不重複的similar_pairs。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l = len(line_sim.collect())\n",
    "similar_mul = line_sim.take(l)\n",
    "similar_pair = []\n",
    "for item in similar_mul:\n",
    "    if item not in similar_pair:\n",
    "        similar_pair.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡將類似於signature matrix的lines take出來供mapper9使用，接著將similar_pair做sc.parallelize形成lines並執行flatMap(mapper9)，算出各pair的similarity，由大到小sort後take前10個即是我們要的答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "signature = line_min.take(101)\n",
    "similar_lines = sc.parallelize(similar_pair)\n",
    "similar_lines = similar_lines.flatMap(mapper9)\n",
    "similar_lines = similar_lines.sortByKey(ascending=False)\n",
    "final = similar_lines.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡print出前十個最高的Similarity及其pair。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar pair: [12, 20] Similarity: 1.0\n",
      "Similar pair: [52, 84] Similarity: 1.0\n",
      "Similar pair: [30, 35] Similarity: 0.82\n",
      "Similar pair: [47, 49] Similarity: 0.72\n",
      "Similar pair: [23, 38] Similarity: 0.57\n",
      "Similar pair: [48, 49] Similarity: 0.57\n",
      "Similar pair: [49, 88] Similarity: 0.44\n",
      "Similar pair: [14, 40] Similarity: 0.38\n",
      "Similar pair: [47, 48] Similarity: 0.37\n",
      "Similar pair: [47, 88] Similarity: 0.35\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"Similar pair:\", final[i][1], \"Similarity:\", final[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
